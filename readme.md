# ğŸ“š Local RAG Chatbot for AWS Docs

This project is a **Retrieval-Augmented Generation (RAG)** chatbot you can run entirely on your machine. It can answer context-related questions using your own documents (PDF/TXT) as a knowledge base.

### ğŸ”¹ Features

* Converts PDFs/TXT into embeddings with **FAISS**
* Saves chunk metadata in **PostgreSQL**
* Runs on a **local LLM via Ollama** (with OpenAI as fallback)
* Provides a **Streamlit** chat UI with chat bubbles

### ğŸ›  Tech Stack

* **Ollama** â†’ Run LLM locally without API keys
* **Streamlit** â†’ Interactive web chat interface
* **Python** â†’ RAG pipeline for loading, embedding, searching, and generating answers
* **PostgreSQL** â†’ Store metadata for document chunks

ğŸ’¡ *RAG allows an LLM to use external sources (like PDFs) to give accurate, context-aware answers.*

---

## ğŸ“¦ 1. Requirements

Make sure you have:

* **WSL** (Windows users) â†’ [Install guide](https://learn.microsoft.com/en-us/windows/wsl/install)
* **Git**

  * Linux/WSL â†’ [Guide](https://learn.microsoft.com/en-us/windows/wsl/tutorials/wsl-git)
  * Mac â†’ `brew install git`
* **Docker Desktop**

  * [Windows](https://docs.docker.com/desktop/setup/install/windows-install/)
  * [Linux](https://docs.docker.com/desktop/setup/install/linux/ubuntu/)
  * [Mac](https://docs.docker.com/desktop/setup/install/mac-install/)
* **Docker CLI** â†’ [Linux Install](https://docs.docker.com/engine/install/ubuntu/) or `brew install --cask docker` on Mac
* **[uv](https://github.com/astral-sh/uv)** (Python package manager)
* **[Ollama](https://ollama.com/download)** installed & running locally

---

## ğŸ“‚ 2. Project Structure

```
document-chatbot/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ ingest.py         # PDF/TXT â†’ chunks â†’ embeddings
â”‚   â”œâ”€â”€ search.py         # Retrieve from FAISS + PostgreSQL
â”‚   â”œâ”€â”€ rag_chain.py      # RAG logic (Ollama â†’ OpenAI fallback)
â”‚   â”œâ”€â”€ ui.py             # Streamlit chat interface
â”‚   â””â”€â”€ db_migration.py   # Create/drop tables
â”œâ”€â”€ data/raw/             # Store your documents here
â”œâ”€â”€ models/faiss_index/   # Persisted FAISS index file
â”œâ”€â”€ init.sh               # Wait for Postgres & run migrations
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš¡ 3. Installation

**1ï¸âƒ£ Clone the repo**

```bash
git clone https://github.com/yourname/document-chatbot.git
cd document-chatbot
```

**2ï¸âƒ£ Create & activate virtual environment**

```bash
uv venv venv
source venv/bin/activate
```

**3ï¸âƒ£ Install dependencies**

```bash
uv pip install -r requirements.txt
```

---

## ğŸ¦™ 4. Install & Run Ollama

Start Ollama:

```bash
ollama serve
```

Pull a model (example: llama3):

```bash
ollama pull llama3
```

Test it:

```bash
ollama run llama3 "Hello!"
```

---

## âš™ 5. Configure Environment

Create `config.env`:

```env
PG_HOST=localhost
PG_USER=chatbot
PG_PASSWORD=chatbot
PG_DATABASE=chatbot_db
PG_PORT=5432

OLLAMA_HOST="http://localhost:11434"
OLLAMA_MODEL="llama3"
OPENAI_API_KEY="your-openai-key"
```

---

## ğŸ³ 6. Start PostgreSQL with Docker

```bash
docker-compose --env-file config.env up -d
```

Run database migrations:

```bash
python3 app/db_migration.py up
```

---

## ğŸ“„ 7. Prepare Documents

Put your PDFs/TXT files in:

```
data/raw/
```

Example:

```
data/raw/aws_rag_guide.pdf
```

---

## ğŸ” 8. Ingest Documents

```bash
python3 app/ingest.py
```

This will:

* Read docs
* Chunk text
* Create embeddings
* Store in FAISS + PostgreSQL

---

## ğŸ’¬ 9. Launch the Chatbot

```bash
streamlit run app/ui.py --server.port=8501
```

Open: [http://localhost:8501](http://localhost:8501)

---

Do you want me to also add **a â€œQuick Start with Dockerâ€ section** so beginners can run the entire stack (Postgres + chatbot UI) without installing Python locally? That would make the repo very plug-and-play.


## ğŸ›  10. Troubleshooting

* **Ollama not running?** â†’ Run `ollama serve` before starting Streamlit.
* **Model not found?** â†’ Run `ollama pull llama3` (or whichever model you want).
* **Slow responses?** â†’ Try a smaller model like `mistral` instead of `llama3`.

---

## ğŸ“œ Example curl to Ollama

If you want to send requests manually:

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "llama3",
  "prompt": "Explain RAG in simple words"
}'
```
